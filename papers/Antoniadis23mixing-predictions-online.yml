title: Mixing predictions for online metric algorithms
authors: Antoniadis, Coester, Eliáš, Polak, Simon
abstract: A major technique in learning-augmented online algorithms is combining multiple
  algorithms or predictors. Since the performance of each predictor may vary over
  time, it is desirable to use not the single best predictor as a benchmark, but rather
  a dynamic combination which follows different predictors at different times. We
  design algorithms that combine predictions and are competitive against such dynamic
  combinations for a wide class of online problems, namely, metrical task systems.
  Against the best (in hindsight) unconstrained combination of $\ell$ predictors,
  we obtain a competitive ratio of $O(\ell^2)$, and show that this is best possible.
  However, for a benchmark with slightly constrained number of switches between different
  predictors, we can get a $(1+\epsilon)$-competitive algorithm. Moreover, our algorithms
  can be adapted to access predictors in a bandit-like fashion, querying only one
  predictor at a time. An unexpected implication of one of our lower bounds is a new
  structural insight about covering formulations for the $k$-server problem.
labels:
- k-server / MTS
- multiple predictions
- online
publications:
- name: ICML
  url: https://proceedings.mlr.press/v202/antoniadis23b.html
  year: 2023
  dblp_key: conf/icml/0001C00S23
  bibtex: "@inproceedings{DBLP:conf/icml/0001C00S23,\n  author       = {Antonios Antoniadis\
    \ and\n  Christian Coester and\n  Marek Eli{\\'{a}}s and\n  Adam Polak and\n \
    \ Bertrand Simon},\n  title        = {Mixing Predictions for Online Metric Algorithms},\n\
    \  booktitle    = {{ICML}},\n  series       = {Proceedings of Machine Learning\
    \ Research},\n  volume       = {202},\n  pages        = {969--983},\n  publisher\
    \    = {{PMLR}},\n  year         = {2023}\n}\n"
- name: arXiv
  url: https://arxiv.org/abs/2304.01781
  year: 2023
  month: 4
  day: 4
