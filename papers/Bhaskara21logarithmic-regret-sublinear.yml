title: Logarithmic Regret from Sublinear Hints
authors: Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit
abstract: We consider the online linear optimization problem, where at every step the algorithm plays a point $x_t$ in the
  unit ball, and suffers loss $\langle c_t, x_t\rangle$ for some cost vector $c_t$ that is then revealed to the algorithm.
  Recent work showed that if an algorithm receives a hint $h_t$ that has non-trivial correlation with $c_t$ before it plays
  $x_t$, then it can achieve a regret guarantee of $O(\log T)$, improving on the bound of $\Theta(\sqrt{T})$ in the standard
  setting. In this work, we study the question of whether an algorithm really requires a hint at every time step. Somewhat
  surprisingly, we show that an algorithm can obtain $O(\log T)$ regret with just $O(\sqrt{T})$ hints under a natural query
  model; in contrast, we also show that $o(\sqrt{T})$ hints cannot guarantee better than $\Omega(\sqrt{T})$ regret. We give
  two applications of our result, to the well-studied setting of optimistic regret bounds and to the problem of online learning
  with abstention.
labels:
- learning
- online
publications:
- name: NeurIPS
  url: https://proceedings.neurips.cc/paper/2021/file/edb947f2bbceb132245fdde9c59d3f59-Paper.pdf
  year: 2021
  dblp_key: conf/nips/BhaskaraCKP21
  bibtex: "@inproceedings{DBLP:conf/nips/BhaskaraCKP21,\n  author       = {Aditya Bhaskara and\n  Ashok Cutkosky and\n  Ravi\
    \ Kumar and\n  Manish Purohit},\n  title        = {Logarithmic Regret from Sublinear Hints},\n  booktitle    = {NeurIPS},\n\
    \  pages        = {28222--28232},\n  year         = {2021}\n}\n"
- name: arXiv
  url: https://arxiv.org/abs/2111.05257
  year: 2021
  month: 11
  day: 9
year: 2021
